{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "product_urls = []\n",
    "list_of_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each page page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,252):\n",
    "    urls.append(f\"https://www.etsy.com/in-en/c/jewelry/earrings/ear-jackets-and-climbers?ref=pagination&explicit=1&page={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping each product's urls | 16,064 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    driver = webdriver.Chrome(executable_path=\"/Users/nallu/chromedriver\")\n",
    "    driver.get(url)\n",
    "    sleep(5)\n",
    "    for i in range(1, 65):\n",
    "        product = driver.find_element_by_xpath(f'//*[@id=\"content\"]/div/div[1]/div/div[3]/div[2]/div[2]/div[2]/div/div/ul/li[{i}]/div/a')\n",
    "        product_urls.append(product.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping each product's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='/Users/nallu/chromedriver')  \n",
    "for product_url in product_urls[1:]:\n",
    "    try:\n",
    "        driver.get(product_url)\n",
    "        sleep(5)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,'html')\n",
    "        for i in range(4):\n",
    "            try:\n",
    "                list_of_reviews.append(soup.select(f'#review-preview-toggle-{i}')[0].getText().strip())\n",
    "            except:\n",
    "                continue\n",
    "        while(True):\n",
    "            try:\n",
    "                next_button = driver.find_element_by_xpath('//*[@id=\"reviews\"]/div[2]/nav/ul/li[position() = last()]/a[contains(@href, \"https\")]')\n",
    "                if next_button != None:\n",
    "                    next_button.click()\n",
    "                    sleep(5)\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html,'html')\n",
    "                    for i in range(4):\n",
    "                        try: \n",
    "                            list_of_reviews.append(soup.select(f'#review-preview-toggle-{i}')[0].getText().strip())\n",
    "                        except:\n",
    "                            continue\n",
    "            except Exception as e:\n",
    "                print('finsish : ', e)\n",
    "                break\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappedReviewsAll = pd.DataFrame(list_of_reviews, index = None, columns = ['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappedReviewsAll.to_csv('scrappedReviewsAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scrappedReviewsAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect('scrappedReviewsAll.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('scrappedReviewsAlldb', conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
